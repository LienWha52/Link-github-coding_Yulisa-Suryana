# -*- coding: utf-8 -*-
"""klasifikasi_knn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMpr_sTevW492UMkqp3sRM25Z6vkJQXc
"""

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Load your CSV data
df = pd.read_csv('dataset_banjir_fixx.csv')

df

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Load the dataset
data = df

# Separate the features (X) and the target (y)
X = data.drop(columns=['flood', 'date', 'station_name'])  # Replace 'target' with the name of your target column
y = data['flood']  # Replace 'target' with the name of your target column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Combine the resampled data into a single DataFrame
resampled_data = pd.concat([pd.DataFrame(X_train_resampled, columns=X.columns), pd.DataFrame(y_train_resampled, columns=['flood'])], axis=1)

print("Original dataset shape:", y.value_counts())
print("Resampled dataset shape:", y_train_resampled.value_counts())

pd.value_counts(y).plot.bar()
plt.title('Sebelum SMOTE')
plt.xticks(rotation=0)
plt.xlabel('Class')
plt.ylabel('Frequency')
y.value_counts()

pd.value_counts(y_train_resampled).plot.bar()
plt.title('Sesudah SMOTE')
plt.xticks(rotation=0)
plt.xlabel('Class')
plt.ylabel('Frequency')
pd.value_counts(y_train_resampled)

resampled_data.head()

resampled_data.set_index("id", inplace=True)

data = resampled_data

data.head()

# Assuming your target column is named 'target'
X = data.drop('flood', axis=1)
y = data['flood']

X.head()

# Encode categorical variables if necessary
# If your dataset contains categorical variables, you may need to encode them.
# For simplicity, let's assume all features are numeric.

# Split the dataset into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.head()

y_train

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

knn_pred

actual = y_test  # Actual target values
# Create confusion matrices
knn_cm = confusion_matrix(actual, knn_pred)

# Convert confusion matrices to DataFrames
def confusion_matrix_to_dataframe(cm):
    labels = sorted(set(actual))
    df_cm = pd.DataFrame(cm, index=labels, columns=labels)
    df_cm.index.name = 'Actual'
    df_cm.columns.name = 'Predicted'
    return df_cm

# Convert confusion matrices to DataFrames with labels
knn_cm_df = confusion_matrix_to_dataframe(knn_cm)

knn_cm_df

def evaluate(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    return cm, accuracy, precision, recall, f1


# Evaluate KNN
knn_cm, knn_accuracy, knn_precision, knn_recall, knn_f1 = evaluate(y_test, knn_pred)

knn_accuracy

knn.get_params()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# Split data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Normalisasi atau standarisasi data jika diperlukan
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model dan parameter grid
model = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Grid Search dengan Cross-Validation
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_scaled, y_train)

# Menampilkan hasil terbaik
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_}")

# Menggunakan model dengan parameter terbaik untuk prediksi
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)

# Evaluasi performa model
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Assuming your new data has the same features as your original data (except the target)

# Make predictions using the trained models
knn_predictions = knn.predict(X_test)

combined_data = X_test.copy()  # Copy the new dataset
combined_data['date'] = df['date'][:len(combined_data)]
combined_data['station_name'] = df['station_name'][:len(combined_data)]
combined_data.head()

# Add columns for predicted labels
combined_data['KNN_Predictions'] = knn_predictions

combined_data.head()

pd.value_counts(y_test).plot.bar()
plt.title('Perbandingan Flood setelah diklasifikasi')
plt.xticks(rotation=0)
plt.xlabel('Class')
plt.ylabel('Frequency')
pd.value_counts(y_test)

